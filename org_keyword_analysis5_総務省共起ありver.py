import streamlit as st
import pandas as pd
import numpy as np
import datetime as dt
import plotly.express as px
import plotly.graph_objects as go
import networkx as nx
from collections import defaultdict, Counter
import itertools
import re
import math
try:
    import MeCab
    MECAB_AVAILABLE = True
except ImportError:
    MECAB_AVAILABLE = False
    st.warning("MeCabãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å½¢æ…‹ç´ è§£ææ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã¾ã™ã€‚")

# OpenAIè¨­å®š
try:
    from openai import OpenAI
    import json
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    st.warning("OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIæŠ½å‡ºæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="åœ°æ–¹è‡ªæ²»ä½“æ–‡æ›¸åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰ˆï¼‰",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸ“Š åœ°æ–¹è‡ªæ²»ä½“æ–‡æ›¸åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰ˆï¼‰")
st.markdown("---")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'df_loaded' not in st.session_state:
    st.session_state.df_loaded = False
if 'df_with_locations' not in st.session_state:
    st.session_state.df_with_locations = None
if 'custom_keywords' not in st.session_state:
    st.session_state.custom_keywords = []
if 'cooccurrence_data' not in st.session_state:
    st.session_state.cooccurrence_data = None
if 'ai_keywords_cache' not in st.session_state:
    st.session_state.ai_keywords_cache = {}

# ORG_CODE_MAPPINGã®å®šç¾©ï¼ˆçœç•¥ï¼‰
ORG_CODE_MAPPING = {
    # ... çœç•¥ ...
}

# éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã¨åå‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
PREFECTURE_MAPPING = {
    "01": "åŒ—æµ·é“", "02": "é’æ£®çœŒ", "03": "å²©æ‰‹çœŒ", "04": "å®®åŸçœŒ", "05": "ç§‹ç”°çœŒ",
    "06": "å±±å½¢çœŒ", "07": "ç¦å³¶çœŒ", "08": "èŒ¨åŸçœŒ", "09": "æ ƒæœ¨çœŒ", "10": "ç¾¤é¦¬çœŒ",
    "11": "åŸ¼ç‰çœŒ", "12": "åƒè‘‰çœŒ", "13": "æ±äº¬éƒ½", "14": "ç¥å¥ˆå·çœŒ", "15": "æ–°æ½ŸçœŒ",
    "16": "å¯Œå±±çœŒ", "17": "çŸ³å·çœŒ", "18": "ç¦äº•çœŒ", "19": "å±±æ¢¨çœŒ", "20": "é•·é‡çœŒ",
    "21": "å²é˜œçœŒ", "22": "é™å²¡çœŒ", "23": "æ„›çŸ¥çœŒ", "24": "ä¸‰é‡çœŒ", "25": "æ»‹è³€çœŒ",
    "26": "äº¬éƒ½åºœ", "27": "å¤§é˜ªåºœ", "28": "å…µåº«çœŒ", "29": "å¥ˆè‰¯çœŒ", "30": "å’Œæ­Œå±±çœŒ",
    "31": "é³¥å–çœŒ", "32": "å³¶æ ¹çœŒ", "33": "å²¡å±±çœŒ", "34": "åºƒå³¶çœŒ", "35": "å±±å£çœŒ",
    "36": "å¾³å³¶çœŒ", "37": "é¦™å·çœŒ", "38": "æ„›åª›çœŒ", "39": "é«˜çŸ¥çœŒ", "40": "ç¦å²¡çœŒ",
    "41": "ä½è³€çœŒ", "42": "é•·å´çœŒ", "43": "ç†Šæœ¬çœŒ", "44": "å¤§åˆ†çœŒ", "45": "å®®å´çœŒ",
    "46": "é¹¿å…å³¶çœŒ", "47": "æ²–ç¸„çœŒ"
}

# é™¤å¤–ã™ã‚‹ä¸€èˆ¬çš„ãªå˜èªãƒªã‚¹ãƒˆ
EXCLUDE_WORDS = {
    # ä¸€èˆ¬çš„ãªå‹•è©ãƒ»å½¢å®¹è©
    'ã™ã‚‹', 'ã‚ã‚‹', 'ãªã‚‹', 'ã„ã‚‹', 'ã§ãã‚‹', 'ã‚Œã‚‹', 'ã‚‰ã‚Œã‚‹', 'ã‚ˆã‚‹', 'ãŠã‚‹', 'ã„ã†', 'ã‚‚ã¤',
    'ã„ã', 'ãã‚‹', 'ã¿ã‚‹', 'ãŠã“ãªã†', 'è¡Œã†', 'æ€ã†', 'è€ƒãˆã‚‹', 'å‡ºã‚‹', 'å…¥ã‚‹', 'ã‚ã‹ã‚‹',
    # ä¸€èˆ¬çš„ãªåè©
    'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã¨ã“ã‚', 'ã¨ã', 'ã²ã¨', 'äºº', 'æ–¹', 'ã¨ãŠã‚Š', 'ã¾ã¾',
    'ã‚ˆã†', 'ã»ã†', 'ã»ã‹', 'ãã‚Œ', 'ã“ã‚Œ', 'ã‚ã‚Œ', 'ã©ã‚Œ', 'ã“ã“', 'ãã“', 'ã‚ãã“',
    # è¡Œæ”¿ç”¨èªï¼ˆä¸€èˆ¬çš„ã™ãã‚‹ã‚‚ã®ï¼‰
    'æ–½ç­–', 'è¨ˆç”»', 'æ”¿ç­–', 'èª²', 'éƒ¨', 'å±€', 'å®¤', 'ä¿‚', 'æ‹…å½“', 'å®Ÿæ–½', 'æ¨é€²',
    'äº‹æ¥­', 'æ¥­å‹™', 'å–çµ„', 'å–ã‚Šçµ„ã¿', 'å¯¾å¿œ', 'å®Ÿç¾', 'ç¢ºä¿', 'å‘ä¸Š', 'ä¿ƒé€²',
    'å¼·åŒ–', 'å……å®Ÿ', 'æ•´å‚™', 'æ´»ç”¨', 'æ”¯æ´', 'æä¾›', 'æ§‹ç¯‰', 'å½¢æˆ', 'å‰µå‡º',
    # æ¥ç¶šè©ãƒ»åŠ©è©ãªã©
    'ãŠã‚ˆã³', 'ã¾ãŸ', 'ãŸã ã—', 'ãªãŠ', 'ã•ã‚‰ã«', 'ã»ã‹', 'ãªã©', 'ç­‰', 'ã‚ˆã‚Š',
    'ã‹ã‚‰', 'ã¾ã§', 'ã«ã¤ã„ã¦', 'ã«é–¢ã™ã‚‹', 'ã«ãŠã‘ã‚‹', 'ã«ã‚ˆã‚‹', 'ãŸã‚ã®',
    # æ•°å­—ãƒ»è¨˜å·é–¢é€£
    'å¹´', 'æœˆ', 'æ—¥', 'ç¬¬', 'æ¡', 'é …', 'å·', 'ç« ', 'ç¯€', 'æ¬¾', 'ç›®',
    # ãã®ä»–
    'ã‚ã‚Š', 'ãªã—', 'ã§ã', 'ã“ã¡ã‚‰', 'ãã‚Œãã‚Œ', 'å„', 'å½“', 'æœ¬', 'ä»Š', 'æ¬¡'
}

def add_location_columns(df):
    """éƒ½é“åºœçœŒåã¨å¸‚åŒºç”ºæ‘åã®åˆ—ã‚’è¿½åŠ """
    df['prefecture_code'] = df['code'].astype(str).str[:2]
    df['prefecture_name'] = df['prefecture_code'].map(PREFECTURE_MAPPING)
    df['municipality_name'] = df['code'].astype(str).map(ORG_CODE_MAPPING)
    
    # å¸‚åŒºç”ºæ‘åã‹ã‚‰éƒ½é“åºœçœŒåã‚’é™¤å»ï¼ˆéƒ½é“åºœçœŒåºã®å ´åˆã‚’é™¤ãï¼‰
    def clean_municipality_name(row):
        if pd.isna(row['municipality_name']):
            return None
        if row['municipality_name'].endswith('åº'):
            return row['municipality_name']
        return row['municipality_name']
    
    df['municipality_name'] = df.apply(clean_municipality_name, axis=1)
    
    # fiscal_year_startåˆ—ã‹ã‚‰å¹´åº¦ã‚’æŠ½å‡ºï¼ˆæ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†ï¼‰
    try:
        # fiscal_year_startåˆ—ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        df['fiscal_year_str'] = df['fiscal_year_start'].astype(str)
        
        # å¹´åº¦ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°
        def extract_fiscal_year(val):
            if pd.isna(val) or val == 'nan':
                return None
            
            val_str = str(val).strip()
            
            # ã‚«ãƒ³ãƒã‚’é™¤å»
            val_str = val_str.replace(',', '')
            
            # 1. ã¾ãš4æ¡ã®æ•°å­—ã ã‘ã®å ´åˆï¼ˆä¾‹: "2023", "2,023"ï¼‰
            if val_str.isdigit() and len(val_str) == 4:
                year = int(val_str)
                if 1900 <= year <= 2100:
                    return year
            
            # 2. æ—¥ä»˜å½¢å¼ã®å ´åˆï¼ˆä¾‹: "2023-04-01", "2023/4/1"ï¼‰
            import re
            # YYYY-MM-DD ã¾ãŸã¯ YYYY/MM/DD å½¢å¼
            date_match = re.match(r'^(\d{4})[-/](\d{1,2})[-/](\d{1,2})', val_str)
            if date_match:
                year = int(date_match.group(1))
                month = int(date_match.group(2))
                # 4æœˆä»¥é™ãªã‚‰å¹´åº¦ã¯ãã®ã¾ã¾ã€3æœˆä»¥å‰ãªã‚‰å‰å¹´åº¦
                if month >= 4:
                    return year
                else:
                    return year - 1
            
            # 3. å¹´åº¦è¡¨è¨˜ã®å ´åˆï¼ˆä¾‹: "2023å¹´åº¦", "ä»¤å’Œ5å¹´åº¦"ï¼‰
            year_match = re.search(r'(\d{4})å¹´åº¦', val_str)
            if year_match:
                return int(year_match.group(1))
            
            # 4. ãã®ä»–ã®4æ¡æ•°å­—ã‚’å«ã‚€å ´åˆ
            four_digit_match = re.search(r'(\d{4})', val_str)
            if four_digit_match:
                year = int(four_digit_match.group(1))
                if 1900 <= year <= 2100:
                    return year
            
            return None
        
        # å¹´åº¦ã‚’æŠ½å‡º
        df['fiscal_year'] = df['fiscal_year_str'].apply(extract_fiscal_year)
        
        # NaNã®æ•°ã‚’ç¢ºèª
        na_count = df['fiscal_year'].isna().sum()
        if na_count > 0:
            st.info(f"å¹´åº¦ã‚’æŠ½å‡ºã§ããªã‹ã£ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ{na_count}ä»¶ã‚ã‚Šã¾ã™ã€‚")
        
        # Int64å‹ã«å¤‰æ›ï¼ˆNaNã‚’å«ã‚€å ´åˆï¼‰
        df['fiscal_year'] = df['fiscal_year'].astype('Int64')
        
    except Exception as e:
        st.error(f"å¹´åº¦ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã¦æ•°å€¤ã«å¤‰æ›
        try:
            df['fiscal_year'] = df['fiscal_year_start'].astype(str).str.replace(',', '').astype(float).astype('Int64')
        except:
            df['fiscal_year'] = pd.NA
    
    return df

def calculate_word_importance_score(word, freq, total_docs):
    """
    å˜èªã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    - è¤‡åˆèªã‚„é•·ã„å˜èªã‚’å„ªå…ˆ
    - é »åº¦ã ã‘ã§ãªãå˜èªã®ç‰¹æ€§ã‚‚è€ƒæ…®
    """
    # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆé »åº¦ã®å¯¾æ•°ï¼‰
    base_score = math.log(freq + 1)
    
    # é•·ã•ãƒœãƒ¼ãƒŠã‚¹
    length_bonus = 1.0
    if len(word) >= 4:
        length_bonus = 1.5
    if len(word) >= 6:
        length_bonus = 2.0
    if len(word) >= 8:
        length_bonus = 3.0
    
    # æ–‡å­—ç¨®ã®è¤‡é›‘ã•ãƒœãƒ¼ãƒŠã‚¹
    has_katakana = any('ã‚¡' <= c <= 'ãƒ¶' for c in word)
    has_kanji = any('\u4e00' <= c <= '\u9fff' for c in word)
    has_alpha = any(c.isalpha() and ord(c) < 128 for c in word)
    
    complexity_bonus = 1.0
    char_types = sum([has_katakana, has_kanji, has_alpha])
    if char_types >= 2:
        complexity_bonus = 1.5
    
    # çŸ­ã„å˜èªã«ãƒšãƒŠãƒ«ãƒ†ã‚£
    short_penalty = 1.0
    if len(word) <= 2:
        short_penalty = 0.2
    elif len(word) == 3:
        short_penalty = 0.5
    
    # ç·åˆã‚¹ã‚³ã‚¢
    importance_score = (
        base_score * 
        length_bonus * 
        complexity_bonus * 
        short_penalty
    )
    
    return importance_score

def extract_keywords_with_ai(text, api_key, max_keywords=30, sample_mode=False):
    """
    AIã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹
    
    Parameters:
    - text: åˆ†æå¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
    - api_key: OpenAI APIã‚­ãƒ¼
    - max_keywords: æŠ½å‡ºã™ã‚‹æœ€å¤§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°
    - sample_mode: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    """
    if not OPENAI_AVAILABLE or not api_key:
        return []
    
    try:
        client = OpenAI(api_key=api_key)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‚’çŸ­ç¸®
        if sample_mode:
            # ãƒ†ã‚­ã‚¹ãƒˆã®æœ€åˆã€ä¸­é–“ã€æœ€å¾Œã‹ã‚‰æŠ½å‡º
            if len(text) > 3000:
                parts = []
                parts.append(text[:1000])  # æœ€åˆã®1000æ–‡å­—
                mid_start = len(text) // 2 - 500
                parts.append(text[mid_start:mid_start + 1000])  # ä¸­é–“ã®1000æ–‡å­—
                parts.append(text[-1000:])  # æœ€å¾Œã®1000æ–‡å­—
                text = ' '.join(parts)
        else:
            # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šãƒ†ã‚­ã‚¹ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚ã‚‹
            if len(text) > 4000:
                text = text[:4000]
        
        prompt = f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€æœ€ã‚‚é‡è¦ã§ç‰¹å¾´çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’{max_keywords}å€‹ã¾ã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

æŠ½å‡ºåŸºæº–ï¼š
1. ãã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸»é¡Œã‚„å†…å®¹ã‚’æœ€ã‚‚ã‚ˆãè¡¨ã™å˜èªãƒ»ãƒ•ãƒ¬ãƒ¼ã‚º
2. å°‚é–€ç”¨èªã€å›ºæœ‰åè©ã€é‡è¦ãªæ¦‚å¿µ
3. è¤‡åˆèªã‚‚ç©æ¥µçš„ã«æŠ½å‡ºï¼ˆä¾‹ï¼šã€Œé–¢ä¿‚äººå£ã€ã€Œåœ°åŸŸæ´»æ€§åŒ–ã€ã€Œãƒ‡ã‚¸ã‚¿ãƒ«ç”°åœ’éƒ½å¸‚ã€ãªã©ï¼‰
4. ä¸€èˆ¬çš„ã™ãã‚‹å˜èªï¼ˆã™ã‚‹ã€ã‚ã‚‹ã€ã“ã¨ã€ãªã©ï¼‰ã¯é™¤å¤–
5. ã€Œæ–½ç­–ã€ã€Œè¨ˆç”»ã€ã€Œæ”¿ç­–ã€ã€Œèª²ã€ãªã©ã®ä¸€èˆ¬çš„ãªè¡Œæ”¿ç”¨èªã¯é™¤å¤–
6. ãã®ãƒ†ã‚­ã‚¹ãƒˆç‰¹æœ‰ã®ã€ä»–ã¨å·®åˆ¥åŒ–ã§ãã‚‹ç‰¹å¾´çš„ãªç”¨èªã‚’å„ªå…ˆ

å‡ºåŠ›å½¢å¼ï¼š
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1è¡Œã«1ã¤ãšã¤ã€æ”¹è¡Œã§åŒºåˆ‡ã£ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚
ç•ªå·ã‚„è¨˜å·ã¯ä»˜ã‘ãªã„ã§ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆï¼š
{text}
"""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªã®ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚åœ°æ–¹è‡ªæ²»ä½“ã®æ–‡æ›¸ã‹ã‚‰ç‰¹å¾´çš„ã§é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=800
        )
        
        result = response.choices[0].message.content
        
        # æ”¹è¡Œã§åˆ†å‰²ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        keywords = []
        for line in result.split('\n'):
            line = line.strip()
            # ç©ºè¡Œã€ç•ªå·ä»˜ãè¡Œã€ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»
            line = re.sub(r'^[\d\.\-\*\ãƒ»]+\s*', '', line)
            line = re.sub(r'[{}\[\]"\'`]', '', line)
            
            # JSONã£ã½ã„æ–‡å­—åˆ—ã‚’é™¤å¤–
            if line and len(line) >= 2 and not any(x in line.lower() for x in ['json', 'keywords', '...', '{', '}', '[', ']']):
                # æœ€çµ‚çš„ãªé™¤å¤–ãƒã‚§ãƒƒã‚¯
                if line not in EXCLUDE_WORDS:
                    keywords.append(line)
        
        return keywords[:max_keywords]
        
    except Exception as e:
        st.error(f"AIæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return []

def tokenize_text_batch(texts, use_mecab=True, use_compound=True):
    """è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§å˜èªã«åˆ†è§£ï¼ˆãƒãƒƒãƒå‡¦ç†ç”¨ï¼‰"""
    results = []
    
    if use_mecab and MECAB_AVAILABLE:
        try:
            tagger = MeCab.Tagger()
            tagger.parse('')  # åˆæœŸåŒ–ï¼ˆä¸€åº¦ã ã‘ï¼‰
            
            for text in texts:
                if pd.isna(text) or text == '':
                    results.append([])
                    continue
                
                text = str(text)
                words = []
                compounds = []
                
                node = tagger.parseToNode(text)
                prev_node = None
                
                while node:
                    features = node.feature.split(',')
                    pos = features[0]
                    
                    if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©']:
                        word = features[6] if len(features) > 6 and features[6] != '*' else node.surface
                        
                        if word not in EXCLUDE_WORDS and len(word) > 1 and not word.isdigit():
                            words.append(word)
                            
                            if use_compound and pos == 'åè©' and prev_node:
                                prev_features = prev_node.feature.split(',')
                                if prev_features[0] == 'åè©':
                                    prev_word = prev_features[6] if len(prev_features) > 6 and prev_features[6] != '*' else prev_node.surface
                                    if prev_word not in EXCLUDE_WORDS and len(prev_word) > 1:
                                        compound = prev_word + word
                                        if len(compound) <= 10:
                                            compounds.append(compound)
                    
                    prev_node = node if pos == 'åè©' else None
                    node = node.next
                
                if use_compound:
                    words.extend(list(set(compounds)))
                
                results.append(words)
        except:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç°¡æ˜“è§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            for text in texts:
                results.append(tokenize_text(text, use_mecab=False))
    else:
        # ç°¡æ˜“è§£æ
        for text in texts:
            if pd.isna(text) or text == '':
                results.append([])
                continue
            
            text = str(text)
            pattern = r'[ã‚¡-ãƒ´ãƒ¼]+|[ã-ã‚“]+|[ä¸€-é¾¥]+|[a-zA-Z]+'
            words = re.findall(pattern, text)
            words = [w for w in words if w not in EXCLUDE_WORDS and len(w) > 1]
            results.append(words)
    
    return results

def calculate_cooccurrence(df, min_count=5, top_n_words=100, use_ai=False, api_key=None, sample_size=None):
    """å…±èµ·é »åº¦ã‚’è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
    import time
    start_time = time.time()
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å®Ÿæ–½
    if sample_size and len(df) > sample_size:
        st.info(f"ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„ãŸã‚ã€{sample_size}ä»¶ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
        df = df.sample(n=sample_size, random_state=42)
    
    # å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    word_counts = Counter()
    # å…±èµ·å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    cooccurrence_counts = defaultdict(int)
    
    progress_bar = st.progress(0)
    total_docs = len(df)
    
    if use_ai and api_key:
        # AIæŠ½å‡ºï¼ˆãƒãƒƒãƒå‡¦ç†ã¯é›£ã—ã„ã®ã§å€‹åˆ¥å‡¦ç†ï¼‰
        processed_count = 0
        for idx, row in df.iterrows():
            processed_count += 1
            if processed_count % 10 == 0:
                progress_bar.progress(processed_count / total_docs)
            
            text = row['content_text']
            text_hash = hash(text[:100])
            
            if text_hash in st.session_state.ai_keywords_cache:
                words = st.session_state.ai_keywords_cache[text_hash]
            else:
                words = extract_keywords_with_ai(text, api_key, sample_mode=True)
                st.session_state.ai_keywords_cache[text_hash] = words
            
            unique_words = list(set(words))
            for word in unique_words:
                word_counts[word] += 1
            
            for word1, word2 in itertools.combinations(sorted(unique_words), 2):
                cooccurrence_counts[(word1, word2)] += 1
    else:
        # é€šå¸¸ã®å½¢æ…‹ç´ è§£æï¼ˆãƒãƒƒãƒå‡¦ç†ã§é«˜é€ŸåŒ–ï¼‰
        batch_size = 100
        all_words_list = []
        
        # ãƒãƒƒãƒå‡¦ç†ã§å½¢æ…‹ç´ è§£æ
        for i in range(0, len(df), batch_size):
            batch_df = df.iloc[i:i+batch_size]
            texts = batch_df['content_text'].tolist()
            
            # ãƒãƒƒãƒã§å½¢æ…‹ç´ è§£æ
            batch_words = tokenize_text_batch(texts, use_mecab=MECAB_AVAILABLE, use_compound=True)
            all_words_list.extend(batch_words)
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°
            progress = min((i + batch_size) / total_docs, 1.0)
            progress_bar.progress(progress)
        
        # å˜èªã‚«ã‚¦ãƒ³ãƒˆã¨å…±èµ·è¨ˆç®—
        for words in all_words_list:
            unique_words = list(set(words))
            
            # å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for word in unique_words:
                word_counts[word] += 1
            
            # å…±èµ·å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆNumPyã§é«˜é€ŸåŒ–ï¼‰
            if len(unique_words) > 1:
                sorted_words = sorted(unique_words)
                for i in range(len(sorted_words)):
                    for j in range(i + 1, len(sorted_words)):
                        cooccurrence_counts[(sorted_words[i], sorted_words[j])] += 1
    
    progress_bar.empty()
    
    # å‡¦ç†æ™‚é–“ã‚’è¡¨ç¤º
    elapsed_time = time.time() - start_time
    st.info(f"å½¢æ…‹ç´ è§£æå®Œäº†: {elapsed_time:.1f}ç§’")
    
    # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§é«˜é€ŸåŒ–ï¼‰
    word_list = []
    count_list = []
    score_list = []
    
    for word, count in word_counts.items():
        if count >= min_count:
            word_list.append(word)
            count_list.append(count)
            score_list.append(calculate_word_importance_score(word, count, total_docs))
    
    # NumPyé…åˆ—ã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
    import numpy as np
    scores = np.array(score_list)
    sorted_indices = np.argsort(scores)[::-1][:top_n_words]
    
    # ä¸Šä½Nèªã‚’é¸æŠ
    top_words = [word_list[i] for i in sorted_indices]
    top_words_set = set(top_words)
    
    # å…±èµ·ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé«˜é€ŸåŒ–ï¼‰
    filtered_cooccurrence = {
        key: count 
        for key, count in cooccurrence_counts.items()
        if key[0] in top_words_set and key[1] in top_words_set and count >= min_count
    }
    
    # é¸æŠã•ã‚ŒãŸå˜èªã®ã‚«ã‚¦ãƒ³ãƒˆã®ã¿ã‚’ä¿æŒ
    filtered_word_counts = {word: word_counts[word] for word in top_words}
    
    return filtered_word_counts, filtered_cooccurrence, top_words

def create_cooccurrence_network(word_counts, cooccurrence_data, top_words, layout_type='spring', 
                              community_resolution=1.0, edge_threshold=0.5):
    """
    å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ
    
    Parameters:
    - word_counts: å˜èªã®å‡ºç¾å›æ•°
    - cooccurrence_data: å…±èµ·ãƒ‡ãƒ¼ã‚¿
    - top_words: è¡¨ç¤ºã™ã‚‹å˜èªãƒªã‚¹ãƒˆ
    - layout_type: ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚¿ã‚¤ãƒ—
    - community_resolution: ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºã®è§£åƒåº¦ï¼ˆé«˜ã„ã»ã©å°ã•ãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰
    - edge_threshold: ã‚¨ãƒƒã‚¸è¡¨ç¤ºã®é–¾å€¤ï¼ˆ0-1ï¼‰
    """
    # NetworkXã‚°ãƒ©ãƒ•ã®ä½œæˆ
    G = nx.Graph()
    
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
    for word in top_words:
        G.add_node(word, count=word_counts[word])
    
    # ã‚¨ãƒƒã‚¸ã®è¿½åŠ ï¼ˆé‡ã¿ã®æ­£è¦åŒ–ï¼‰
    edge_weights = []
    for (word1, word2), count in cooccurrence_data.items():
        # æ­£è¦åŒ–ã•ã‚ŒãŸé‡ã¿ï¼ˆJaccardä¿‚æ•°çš„ãªè¨ˆç®—ï¼‰
        weight = count / (word_counts[word1] + word_counts[word2] - count)
        G.add_edge(word1, word2, weight=weight, raw_count=count)
        edge_weights.append(weight)
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡º
    try:
        import community as community_louvain
        partition = community_louvain.best_partition(G, resolution=community_resolution)
    except:
        # community-louvainãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
        partition = {node: 0 for node in G.nodes()}
        st.warning("ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã«ãƒãƒ¼ãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    communities = defaultdict(list)
    for node, comm_id in partition.items():
        communities[comm_id].append(node)
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®è¨ˆç®—ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚’è€ƒæ…®ï¼‰
    if layout_type == 'spring':
        # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã®åˆæœŸä½ç½®ã‚’è¨­å®š
        pos_init = {}
        num_communities = len(communities)
        
        # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ä¸­å¿ƒã‚’å††å‘¨ä¸Šã«é…ç½®
        community_centers = {}
        for i, comm_id in enumerate(communities.keys()):
            angle = 2 * np.pi * i / num_communities
            center_x = 2 * np.cos(angle)
            center_y = 2 * np.sin(angle)
            community_centers[comm_id] = (center_x, center_y)
            
            # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã®ãƒãƒ¼ãƒ‰ã‚’ä¸­å¿ƒã®å‘¨ã‚Šã«é…ç½®
            nodes = communities[comm_id]
            for j, node in enumerate(nodes):
                if len(nodes) == 1:
                    pos_init[node] = (center_x, center_y)
                else:
                    sub_angle = 2 * np.pi * j / len(nodes)
                    radius = 0.5
                    pos_init[node] = (
                        center_x + radius * np.cos(sub_angle),
                        center_y + radius * np.sin(sub_angle)
                    )
        
        pos = nx.spring_layout(G, pos=pos_init, k=2, iterations=50, weight='weight')
    elif layout_type == 'circular':
        pos = nx.circular_layout(G)
    elif layout_type == 'kamada_kawai':
        pos = nx.kamada_kawai_layout(G)
    else:
        pos = nx.random_layout(G, seed=42)
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚«ãƒ©ãƒ¼ã®è¨­å®š
    colors = px.colors.qualitative.Plotly + px.colors.qualitative.Set3
    community_colors = {}
    for i, comm_id in enumerate(sorted(communities.keys())):
        community_colors[comm_id] = colors[i % len(colors)]
    
    # ã‚¨ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…å¤–ã§åŒºåˆ¥ï¼‰
    intra_edges = []  # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã‚¨ãƒƒã‚¸
    inter_edges = []  # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£é–“ã‚¨ãƒƒã‚¸
    
    for edge in G.edges(data=True):
        if edge[2]['weight'] >= edge_threshold * max(edge_weights):
            edge_data = {
                'x': [pos[edge[0]][0], pos[edge[1]][0], None],
                'y': [pos[edge[0]][1], pos[edge[1]][1], None],
                'weight': edge[2]['weight'],
                'count': edge[2]['raw_count']
            }
            
            if partition[edge[0]] == partition[edge[1]]:
                intra_edges.append(edge_data)
            else:
                inter_edges.append(edge_data)
    
    # Plotlyã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä½œæˆ
    traces = []
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£é–“ã‚¨ãƒƒã‚¸ï¼ˆè–„ã„ç·šï¼‰
    for edge in inter_edges:
        traces.append(go.Scatter(
            x=edge['x'], y=edge['y'],
            mode='lines',
            line=dict(width=0.5, color='rgba(200,200,200,0.3)'),
            hoverinfo='text',
            hovertext=f"å…±èµ·å›æ•°: {edge['count']}",
            showlegend=False
        ))
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å†…ã‚¨ãƒƒã‚¸ï¼ˆæ¿ƒã„ç·šï¼‰
    for edge in intra_edges:
        traces.append(go.Scatter(
            x=edge['x'], y=edge['y'],
            mode='lines',
            line=dict(width=edge['weight']*5, color='rgba(100,100,100,0.5)'),
            hoverinfo='text',
            hovertext=f"å…±èµ·å›æ•°: {edge['count']}",
            showlegend=False
        ))
    
    # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã”ã¨ã«ãƒãƒ¼ãƒ‰ã‚’æç”»
    for comm_id, nodes in communities.items():
        node_x = [pos[node][0] for node in nodes]
        node_y = [pos[node][1] for node in nodes]
        node_text = nodes
        node_size = [np.log(G.nodes[node]['count'] + 1) * 10 for node in nodes]
        hover_text = [f"{node}<br>å‡ºç¾å›æ•°: {G.nodes[node]['count']}<br>ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£: {comm_id}" 
                     for node in nodes]
        
        traces.append(go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            name=f'ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ {comm_id}',
            marker=dict(
                size=node_size,
                color=community_colors[comm_id],
                line=dict(width=2, color='white')
            ),
            text=node_text,
            textposition="top center",
            hovertext=hover_text,
            hoverinfo='text'
        ))
    
    # å…¨ä½“ã®çµ±è¨ˆæƒ…å ±
    stats = {
        'num_nodes': len(G.nodes()),
        'num_edges': len(G.edges()),
        'num_communities': len(communities),
        'avg_degree': sum(dict(G.degree()).values()) / len(G.nodes()) if len(G.nodes()) > 0 else 0,
        'density': nx.density(G) if len(G.nodes()) > 1 else 0
    }
    
    return traces, stats, G, partition

def parse_search_query(query):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è§£æã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æ¤œç´¢ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡º"""
    import re
    
    # ANDæ¤œç´¢ã®ãƒã‚§ãƒƒã‚¯
    if ' AND ' in query:
        keywords = []
        parts = query.split(' AND ')
        for part in parts:
            part = part.strip()
            # ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å®Œå…¨ä¸€è‡´
            if part.startswith('"') and part.endswith('"'):
                keywords.append(('exact', part[1:-1]))
            else:
                keywords.append(('partial', part))
        return 'AND', keywords
    
    # ORæ¤œç´¢ã®ãƒã‚§ãƒƒã‚¯
    elif ' OR ' in query:
        keywords = []
        parts = query.split(' OR ')
        for part in parts:
            part = part.strip()
            # ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯å®Œå…¨ä¸€è‡´
            if part.startswith('"') and part.endswith('"'):
                keywords.append(('exact', part[1:-1]))
            else:
                keywords.append(('partial', part))
        return 'OR', keywords
    
    # å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    else:
        if query.startswith('"') and query.endswith('"'):
            return 'SINGLE', [('exact', query[1:-1])]
        else:
            return 'SINGLE', [('partial', query)]

def count_keyword_occurrences(df, keyword):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    import re
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è§£æ
    search_type, keywords = parse_search_query(keyword)
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’NaNå¯¾å¿œ
    df['content_text_safe'] = df['content_text'].fillna('')
    
    if search_type == 'SINGLE':
        match_type, kw = keywords[0]
        if match_type == 'exact':
            # å®Œå…¨ä¸€è‡´ï¼ˆå˜èªå¢ƒç•Œã‚’è€ƒæ…®ï¼‰
            pattern = r'\b' + re.escape(kw) + r'\b'
            df['keyword_count'] = df['content_text_safe'].str.count(pattern)
            df['has_keyword'] = df['keyword_count'] > 0
        else:
            # éƒ¨åˆ†ä¸€è‡´
            df['keyword_count'] = df['content_text_safe'].str.count(re.escape(kw))
            df['has_keyword'] = df['keyword_count'] > 0
    
    elif search_type == 'AND':
        # ANDæ¤œç´¢ï¼šã™ã¹ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆã®ã¿ã‚«ã‚¦ãƒ³ãƒˆ
        df['keyword_count'] = 0
        df['has_keyword'] = True
        
        for match_type, kw in keywords:
            if match_type == 'exact':
                pattern = r'\b' + re.escape(kw) + r'\b'
                temp_count = df['content_text_safe'].str.count(pattern)
            else:
                temp_count = df['content_text_safe'].str.count(re.escape(kw))
            
            df['keyword_count'] += temp_count
            df['has_keyword'] &= (temp_count > 0)
        
        # ANDæ¡ä»¶ã‚’æº€ãŸã•ãªã„è¡Œã¯ã‚«ã‚¦ãƒ³ãƒˆã‚’0ã«ã™ã‚‹
        df.loc[~df['has_keyword'], 'keyword_count'] = 0
    
    elif search_type == 'OR':
        # ORæ¤œç´¢ï¼šã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆã«ã‚«ã‚¦ãƒ³ãƒˆ
        df['keyword_count'] = 0
        df['has_keyword'] = False
        
        for match_type, kw in keywords:
            if match_type == 'exact':
                pattern = r'\b' + re.escape(kw) + r'\b'
                temp_count = df['content_text_safe'].str.count(pattern)
            else:
                temp_count = df['content_text_safe'].str.count(re.escape(kw))
            
            df['keyword_count'] += temp_count
            df['has_keyword'] |= (temp_count > 0)
    
    # ä¸€æ™‚çš„ãªåˆ—ã‚’å‰Šé™¤
    df = df.drop('content_text_safe', axis=1)
    
    return df

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("è¨­å®š")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=['csv'])
    
    # AIè¨­å®šï¼ˆå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨ï¼‰
    if OPENAI_AVAILABLE:
        st.header("AIè¨­å®šï¼ˆå…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨ï¼‰")
        use_ai_extraction = st.checkbox("AIã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’ä½¿ç”¨", value=False, 
                                      help="OpenAI APIã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã™")
        if use_ai_extraction:
            openai_api_key = st.text_input("OpenAI API Key", type="password",
                                          help="OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        else:
            openai_api_key = None
            
        # é«˜é€ŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.header("é«˜é€ŸåŒ–è¨­å®š")
        enable_cache = st.checkbox("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ–", value=True,
                                 help="ä¸€åº¦å‡¦ç†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨ã—ã¾ã™")
        if not enable_cache:
            st.session_state.ai_keywords_cache = {}
    else:
        use_ai_extraction = False
        openai_api_key = None
        enable_cache = True
    
    if uploaded_file:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒœã‚¿ãƒ³
        if st.button("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€", type="primary"):
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                # CSVã‚’èª­ã¿è¾¼ã‚€
                try:
                    df = pd.read_csv(uploaded_file)
                except:
                    df = pd.read_csv(uploaded_file, encoding='shift_jis')
                
                # ä½ç½®æƒ…å ±ã‚’è¿½åŠ 
                df = add_location_columns(df)
                st.session_state.df_with_locations = df
                st.session_state.df_loaded = True
                st.success("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
if st.session_state.df_loaded and st.session_state.df_with_locations is not None:
    df = st.session_state.df_with_locations
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.header("ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š")
    
    # æ¤œç´¢æ–¹æ³•ã®èª¬æ˜
    with st.expander("æ¤œç´¢æ–¹æ³•ã®èª¬æ˜"):
        st.markdown("""
        **æ¤œç´¢æ–¹æ³•ï¼š**
        - **éƒ¨åˆ†ä¸€è‡´æ¤œç´¢**: `é£Ÿè‚²` â†’ ã€Œé£Ÿè‚²ã€ã‚’å«ã‚€ã™ã¹ã¦ã®æ–‡å­—åˆ—ã«ãƒãƒƒãƒ
        - **å®Œå…¨ä¸€è‡´æ¤œç´¢**: `"é£Ÿè‚²"` â†’ å˜èªã¨ã—ã¦ç‹¬ç«‹ã—ãŸã€Œé£Ÿè‚²ã€ã®ã¿ã«ãƒãƒƒãƒ
        - **ANDæ¤œç´¢**: `"é£Ÿè‚²" AND "æ•™è‚²"` â†’ ä¸¡æ–¹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸ã®ã¿
        - **ORæ¤œç´¢**: `"é£Ÿè‚²" OR "çµ¦é£Ÿ"` â†’ ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒšãƒ¼ã‚¸
        
        **ä¾‹ï¼š**
        - `ãƒ‡ã‚¸ã‚¿ãƒ«` â†’ ã€Œãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã€ã€Œãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã€ãªã©ã‚‚å«ã‚€
        - `"ãƒ‡ã‚¸ã‚¿ãƒ«"` â†’ å˜èªã¨ã—ã¦ç‹¬ç«‹ã—ãŸã€Œãƒ‡ã‚¸ã‚¿ãƒ«ã€ã®ã¿
        - `"é£Ÿè‚²" AND "çµ¦é£Ÿ"` â†’ ã€Œé£Ÿè‚²ã€ã¨ã€Œçµ¦é£Ÿã€ã®ä¸¡æ–¹ã‚’å«ã‚€ãƒšãƒ¼ã‚¸
        - `é˜²ç½ OR æ¸›ç½` â†’ ã€Œé˜²ç½ã€ã¾ãŸã¯ã€Œæ¸›ç½ã€ã‚’å«ã‚€ãƒšãƒ¼ã‚¸
        """)
    
    col1, col2 = st.columns([3, 1])
    with col1:
        custom_keyword_input = st.text_input(
            "åˆ†æã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
            placeholder='ä¾‹: ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–, "é£Ÿè‚²", "é£Ÿè‚²" AND "æ•™è‚²", é˜²ç½ OR æ¸›ç½'
        )
    with col2:
        if st.button("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿½åŠ ", type="secondary"):
            if custom_keyword_input and custom_keyword_input not in st.session_state.custom_keywords:
                st.session_state.custom_keywords.append(custom_keyword_input)
                st.success(f"ã€Œ{custom_keyword_input}ã€ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            elif custom_keyword_input in st.session_state.custom_keywords:
                st.warning("ã“ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯æ—¢ã«è¿½åŠ ã•ã‚Œã¦ã„ã¾ã™")
    
    # ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆè¡¨ç¤º
    if st.session_state.custom_keywords:
        st.write("**ç™»éŒ²ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:**")
        cols = st.columns(5)
        for i, kw in enumerate(st.session_state.custom_keywords):
            with cols[i % 5]:
                if st.button(f"âŒ {kw}", key=f"del_{i}"):
                    st.session_state.custom_keywords.remove(kw)
                    st.rerun()
    
    st.markdown("---")
    
    # ã‚¿ãƒ–
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“Š åŸºæœ¬çµ±è¨ˆ", "ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ", "ğŸ“ˆ æ™‚ç³»åˆ—åˆ†æ", "ğŸ”— å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ“‘ ãƒ‡ãƒ¼ã‚¿è©³ç´°"])
    
    with tab1:
        st.header("åŸºæœ¬çµ±è¨ˆ")
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ç·æ–‡æ›¸æ•°", f"{len(df):,}")
        with col2:
            st.metric("éƒ½é“åºœçœŒæ•°", df['prefecture_name'].nunique())
        with col3:
            st.metric("å¸‚åŒºç”ºæ‘æ•°", df['municipality_name'].nunique())
        with col4:
            fiscal_year_min = df['fiscal_year'].min()
            fiscal_year_max = df['fiscal_year'].max()
            if pd.notna(fiscal_year_min) and pd.notna(fiscal_year_max):
                st.metric("å¹´åº¦ç¯„å›²", f"{fiscal_year_min}ï½{fiscal_year_max}")
            else:
                st.metric("å¹´åº¦ç¯„å›²", "ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # éƒ½é“åºœçœŒåˆ¥ã®file_idæ•°
        st.subheader("éƒ½é“åºœçœŒåˆ¥ã®æ–‡æ›¸æ•°")
        pref_counts = df.groupby('prefecture_name')['file_id'].nunique().sort_values(ascending=False)
        
        # DataFrameã«å¤‰æ›
        pref_df = pd.DataFrame({
            'éƒ½é“åºœçœŒ': pref_counts.index,
            'æ–‡æ›¸æ•°': pref_counts.values
        })
        
        fig1 = px.bar(
            pref_df,
            x='æ–‡æ›¸æ•°',
            y='éƒ½é“åºœçœŒ',
            orientation='h',
            title="éƒ½é“åºœçœŒåˆ¥ãƒ¦ãƒ‹ãƒ¼ã‚¯æ–‡æ›¸æ•°"
        )
        fig1.update_layout(height=600)
        st.plotly_chart(fig1, use_container_width=True)
        
        # å¸‚åŒºç”ºæ‘åˆ¥ã®file_idæ•°
        st.subheader("å¸‚åŒºç”ºæ‘åˆ¥ã®æ–‡æ›¸æ•°")
        
        # è¡¨ç¤ºä»¶æ•°ã®é¸æŠ
        display_option = st.radio(
            "è¡¨ç¤ºä»¶æ•°",
            options=["ä¸Šä½20ä»¶", "ã™ã¹ã¦è¡¨ç¤º"],
            horizontal=True,
            key="muni_display_option"
        )
        
        muni_counts = df.groupby('municipality_name')['file_id'].nunique().sort_values(ascending=False)
        
        if display_option == "ä¸Šä½20ä»¶":
            muni_counts = muni_counts.head(20)
            title_suffix = "ï¼ˆä¸Šä½20ï¼‰"
        else:
            title_suffix = "ï¼ˆå…¨å¸‚åŒºç”ºæ‘ï¼‰"
        
        # DataFrameã«å¤‰æ›
        muni_df = pd.DataFrame({
            'å¸‚åŒºç”ºæ‘': muni_counts.index,
            'æ–‡æ›¸æ•°': muni_counts.values
        })
        
        fig2 = px.bar(
            muni_df,
            x='æ–‡æ›¸æ•°',
            y='å¸‚åŒºç”ºæ‘',
            orientation='h',
            title=f"å¸‚åŒºç”ºæ‘åˆ¥ãƒ¦ãƒ‹ãƒ¼ã‚¯æ–‡æ›¸æ•°{title_suffix}"
        )
        if display_option == "ã™ã¹ã¦è¡¨ç¤º":
            fig2.update_layout(height=max(600, len(muni_counts) * 20))
        st.plotly_chart(fig2, use_container_width=True)
    
    with tab2:
        st.header("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
        
        if not st.session_state.custom_keywords:
            st.info("ğŸ‘† ä¸Šéƒ¨ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        else:
            # éƒ½é“åºœçœŒãƒ•ã‚£ãƒ«ã‚¿ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸æŠ
            col1, col2 = st.columns([1, 2])
            with col1:
                pref_filter = st.selectbox(
                    "éƒ½é“åºœçœŒã§ãƒ•ã‚£ãƒ«ã‚¿",
                    options=['å…¨å›½'] + sorted(df['prefecture_name'].dropna().unique().tolist()),
                    key="pref_filter_keyword"
                )
            
            with col2:
                selected_keyword = st.selectbox(
                    "åˆ†æã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ",
                    options=st.session_state.custom_keywords,
                    index=0 if st.session_state.custom_keywords else None
                )
            
            if selected_keyword:
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                df_filtered_kw = df.copy()
                if pref_filter != 'å…¨å›½':
                    df_filtered_kw = df_filtered_kw[df_filtered_kw['prefecture_name'] == pref_filter]
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’è¨ˆç®—
                df_with_keyword = count_keyword_occurrences(df_filtered_kw.copy(), selected_keyword)
                
                # åŸºæœ¬æƒ…å ±ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦è¡¨ç¤ºã‚’å¤‰æ›´ï¼‰
                search_type, _ = parse_search_query(selected_keyword)
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    total_occurrences = df_with_keyword['keyword_count'].sum()
                    st.metric("ç·å‡ºç¾å›æ•°", f"{total_occurrences:,}")
                with col2:
                    docs_with_keyword = (df_with_keyword['has_keyword']).sum()
                    st.metric("è©²å½“ãƒšãƒ¼ã‚¸æ•°", f"{docs_with_keyword:,}")
                with col3:
                    st.metric("æ¤œç´¢ã‚¿ã‚¤ãƒ—", search_type)
                
                if pref_filter == 'å…¨å›½':
                    # éƒ½é“åºœçœŒåˆ¥ã®å‡ºç¾å›æ•°
                    st.subheader(f"éƒ½é“åºœçœŒåˆ¥ã€Œ{selected_keyword}ã€å‡ºç¾å›æ•°")
                    pref_keyword_counts = df_with_keyword.groupby('prefecture_name')['keyword_count'].sum()
                    # 0ã‚ˆã‚Šå¤§ãã„å€¤ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    pref_keyword_counts = pref_keyword_counts[pref_keyword_counts > 0].sort_values(ascending=False)
                    
                    if not pref_keyword_counts.empty:
                        # DataFrameã«å¤‰æ›
                        pref_keyword_df = pd.DataFrame({
                            'éƒ½é“åºœçœŒ': pref_keyword_counts.index,
                            'å‡ºç¾å›æ•°': pref_keyword_counts.values
                        })
                        
                        fig3 = px.bar(
                            pref_keyword_df,
                            x='å‡ºç¾å›æ•°',
                            y='éƒ½é“åºœçœŒ',
                            orientation='h',
                            title=f"éƒ½é“åºœçœŒåˆ¥ã€Œ{selected_keyword}ã€å‡ºç¾å›æ•°"
                        )
                        fig3.update_layout(height=600)
                        st.plotly_chart(fig3, use_container_width=True)
                    else:
                        st.info("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                
                # å¸‚åŒºç”ºæ‘åˆ¥ã®å‡ºç¾å›æ•°ã¨ãƒšãƒ¼ã‚¸å‰²åˆ
                st.subheader(f"å¸‚åŒºç”ºæ‘åˆ¥ã€Œ{selected_keyword}ã€å‡ºç¾çŠ¶æ³")
                
                # è¡¨ç¤ºä»¶æ•°ã®é¸æŠ
                display_option_kw = st.radio(
                    "è¡¨ç¤ºä»¶æ•°",
                    options=["ä¸Šä½20ä»¶", "ã™ã¹ã¦è¡¨ç¤º"],
                    horizontal=True,
                    key="muni_display_option_kw"
                )
                
                # å¸‚åŒºç”ºæ‘åˆ¥ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                muni_stats = df_with_keyword.groupby('municipality_name').agg({
                    'keyword_count': 'sum',  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°ã®åˆè¨ˆ
                    'file_id': 'count'  # ç·ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼‰
                }).reset_index()
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå‡ºç¾ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—ï¼ˆhas_keywordãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ï¼‰
                muni_keyword_pages = df_with_keyword[df_with_keyword['has_keyword']].groupby('municipality_name').size().reset_index(name='pages_with_keyword')
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                muni_stats = muni_stats.merge(muni_keyword_pages, on='municipality_name', how='left')
                muni_stats['pages_with_keyword'] = muni_stats['pages_with_keyword'].fillna(0).astype(int)
                
                # ãƒšãƒ¼ã‚¸å‰²åˆã‚’è¨ˆç®—
                muni_stats['page_ratio'] = (muni_stats['pages_with_keyword'] / muni_stats['file_id'] * 100).round(1)
                
                # åˆ—åã‚’å¤‰æ›´
                muni_stats.columns = ['å¸‚åŒºç”ºæ‘', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°', 'ç·ãƒšãƒ¼ã‚¸æ•°', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒšãƒ¼ã‚¸æ•°', 'ãƒšãƒ¼ã‚¸å‰²åˆ(%)']
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆ
                muni_stats = muni_stats[muni_stats['ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°'] > 0].sort_values('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°', ascending=False)
                
                if not muni_stats.empty:
                    if display_option_kw == "ä¸Šä½20ä»¶":
                        muni_stats_display = muni_stats.head(20)
                        title_suffix = "ï¼ˆä¸Šä½20ï¼‰"
                    else:
                        muni_stats_display = muni_stats
                        title_suffix = "ï¼ˆå…¨å¸‚åŒºç”ºæ‘ï¼‰"
                    
                    # ã‚°ãƒ©ãƒ•è¡¨ç¤º
                    fig4 = px.bar(
                        muni_stats_display,
                        x='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°',
                        y='å¸‚åŒºç”ºæ‘',
                        orientation='h',
                        title=f"{'[' + pref_filter + '] ' if pref_filter != 'å…¨å›½' else ''}å¸‚åŒºç”ºæ‘åˆ¥ã€Œ{selected_keyword}ã€å‡ºç¾å›æ•°{title_suffix}",
                        hover_data=['ç·ãƒšãƒ¼ã‚¸æ•°', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒšãƒ¼ã‚¸æ•°', 'ãƒšãƒ¼ã‚¸å‰²åˆ(%)']
                    )
                    if display_option_kw == "ã™ã¹ã¦è¡¨ç¤º":
                        fig4.update_layout(height=max(600, len(muni_stats_display) * 20))
                    st.plotly_chart(fig4, use_container_width=True)
                    
                    # è¡¨å½¢å¼ã§ã‚‚è¡¨ç¤º
                    st.subheader("è©³ç´°ãƒ‡ãƒ¼ã‚¿")
                    st.dataframe(
                        muni_stats_display,
                        use_container_width=True,
                        hide_index=True,
                        column_config={
                            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°": st.column_config.NumberColumn(format="%d"),
                            "ç·ãƒšãƒ¼ã‚¸æ•°": st.column_config.NumberColumn(format="%d"),
                            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒšãƒ¼ã‚¸æ•°": st.column_config.NumberColumn(format="%d"),
                            "ãƒšãƒ¼ã‚¸å‰²åˆ(%)": st.column_config.NumberColumn(format="%.1f%%")
                        }
                    )
                else:
                    st.info("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tab3:
        st.header("æ™‚ç³»åˆ—åˆ†æ")
        
        if not st.session_state.custom_keywords:
            st.info("ğŸ‘† ä¸Šéƒ¨ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        else:
            selected_keyword_ts = st.selectbox(
                "æ™‚ç³»åˆ—åˆ†æã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠ",
                options=st.session_state.custom_keywords,
                index=0 if st.session_state.custom_keywords else None,
                key="keyword_ts"
            )
            
            if selected_keyword_ts:
                # ãƒ‡ãƒ¼ã‚¿ã®å†è¨ˆç®—
                df_ts = count_keyword_occurrences(df.copy(), selected_keyword_ts)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæ¤œç´¢ã‚¿ã‚¤ãƒ—ã‚’è¡¨ç¤ºï¼‰
                search_type_ts, _ = parse_search_query(selected_keyword_ts)
                total_count = df_ts['keyword_count'].sum()
                docs_with_keyword = df_ts['has_keyword'].sum()
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ç·å‡ºç¾å›æ•°", f"{total_count:,}")
                with col2:
                    st.metric("è©²å½“ãƒšãƒ¼ã‚¸æ•°", f"{docs_with_keyword:,}")
                with col3:
                    st.metric("æ¤œç´¢ã‚¿ã‚¤ãƒ—", search_type_ts)
                
                # å¹´åº¦åˆ¥ã®å‡ºç¾å›æ•°
                if total_count > 0:
                    # NaNã‚’é™¤å¤–ã—ã¦ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                    df_ts_valid = df_ts[df_ts['fiscal_year'].notna() & (df_ts['keyword_count'] > 0)]
                    
                    if len(df_ts_valid) > 0:
                        yearly_data = df_ts_valid.groupby('fiscal_year')['keyword_count'].sum()
                        
                        if len(yearly_data) > 0:
                            # DataFrameã«å¤‰æ›
                            yearly_df = pd.DataFrame({
                                'fiscal_year': yearly_data.index,
                                'keyword_count': yearly_data.values
                            })
                            
                            # fiscal_yearãŒæ•°å€¤ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                            yearly_df['fiscal_year'] = pd.to_numeric(yearly_df['fiscal_year'], errors='coerce')
                            yearly_df = yearly_df.dropna(subset=['fiscal_year'])
                            
                            if len(yearly_df) > 0:
                                yearly_df['fiscal_year'] = yearly_df['fiscal_year'].astype(int)
                                yearly_df = yearly_df.sort_values('fiscal_year')
                                
                                # ã‚°ãƒ©ãƒ•ä½œæˆ
                                fig5 = px.line(
                                    yearly_df,
                                    x='fiscal_year',
                                    y='keyword_count',
                                    labels={'fiscal_year': 'å¹´åº¦', 'keyword_count': 'å‡ºç¾å›æ•°'},
                                    title=f"ã€Œ{selected_keyword_ts}ã€ã®å¹´åº¦åˆ¥å‡ºç¾å›æ•°æ¨ç§»",
                                    markers=True
                                )
                                
                                # xè»¸ã®è¨­å®š
                                fig5.update_xaxes(
                                    tickformat='d',
                                    dtick=1,
                                    tickmode='linear'
                                )
                                
                                st.plotly_chart(fig5, use_container_width=True)
                            else:
                                st.warning("æœ‰åŠ¹ãªå¹´åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                        else:
                            st.warning("å¹´åº¦åˆ¥ã®é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        st.warning("æœ‰åŠ¹ãªå¹´åº¦ãƒ‡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.warning(f"ã€Œ{selected_keyword_ts}ã€ã®å‡ºç¾ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                
                st.markdown("---")
                
                # éƒ½é“åºœçœŒé¸æŠ
                selected_pref = st.selectbox(
                    "éƒ½é“åºœçœŒã‚’é¸æŠï¼ˆæ™‚ç³»åˆ—è©³ç´°ï¼‰",
                    options=['å…¨å›½'] + sorted(df['prefecture_name'].dropna().unique().tolist())
                )
                
                if selected_pref != 'å…¨å›½':
                    # é¸æŠã—ãŸéƒ½é“åºœçœŒã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    df_pref = df_ts[df_ts['prefecture_name'] == selected_pref]
                    
                    # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿å‡¦ç†ã‚’ç¶šè¡Œ
                    if len(df_pref) > 0:
                        pref_total = df_pref['keyword_count'].sum()
                        
                        if pref_total > 0:
                            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå‡ºç¾ã—ã¦ã„ã‚‹è¡Œã®ã¿æŠ½å‡º
                            df_pref_with_keyword = df_pref[df_pref['keyword_count'] > 0]
                            
                            # å¹´åº¦åˆ¥ãƒ»å¸‚åŒºç”ºæ‘åˆ¥ã®é›†è¨ˆ
                            muni_yearly = df_pref_with_keyword.groupby(['fiscal_year', 'municipality_name'])['keyword_count'].sum().reset_index()
                            
                            if len(muni_yearly) > 0:
                                # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ
                                pivot_data = muni_yearly.pivot(
                                    index='fiscal_year',
                                    columns='municipality_name',
                                    values='keyword_count'
                                ).fillna(0)
                                
                                # å¸‚åŒºç”ºæ‘ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆå‡ºç¾å›æ•°ã®å¤šã„é †ï¼‰
                                muni_totals = pivot_data.sum().sort_values(ascending=False)
                                available_munis = muni_totals.index.tolist()
                                
                                if len(available_munis) > 0:
                                    # å¸‚åŒºç”ºæ‘é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯èƒ½ï¼‰
                                    st.subheader("è¡¨ç¤ºã™ã‚‹å¸‚åŒºç”ºæ‘ã‚’é¸æŠ")
                                    
                                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¸Šä½5å¸‚åŒºç”ºæ‘ã‚’é¸æŠ
                                    default_munis = available_munis[:5]
                                    
                                    # è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³
                                    col1, col2 = st.columns([3, 1])
                                    with col1:
                                        selected_munis = st.multiselect(
                                            "å¸‚åŒºç”ºæ‘ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
                                            options=available_munis,
                                            default=default_munis,
                                            help="ã‚°ãƒ©ãƒ•ã«è¡¨ç¤ºã™ã‚‹å¸‚åŒºç”ºæ‘ã‚’é¸æŠã—ã¦ãã ã•ã„"
                                        )
                                    with col2:
                                        if st.button("ä¸Šä½5ä»¶ã‚’é¸æŠ", key="select_top5"):
                                            selected_munis = available_munis[:5]
                                            st.rerun()
                                        if st.button("ã™ã¹ã¦é¸æŠ", key="select_all"):
                                            selected_munis = available_munis
                                            st.rerun()
                                    
                                    if selected_munis:
                                        # ã‚°ãƒ©ãƒ•ä½œæˆ
                                        fig6 = go.Figure()
                                        
                                        for muni in selected_munis:
                                            if muni in pivot_data.columns:
                                                fig6.add_trace(go.Scatter(
                                                    x=pivot_data.index,
                                                    y=pivot_data[muni],
                                                    mode='lines+markers',
                                                    name=muni
                                                ))
                                        
                                        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å‹•çš„ã«å¤‰æ›´
                                        if len(selected_munis) == len(available_munis):
                                            title_suffix = "ï¼ˆå…¨å¸‚åŒºç”ºæ‘ï¼‰"
                                        elif len(selected_munis) <= 5:
                                            title_suffix = f"ï¼ˆ{len(selected_munis)}å¸‚åŒºç”ºæ‘ï¼‰"
                                        else:
                                            title_suffix = f"ï¼ˆ{len(selected_munis)}å¸‚åŒºç”ºæ‘ï¼‰"
                                        
                                        fig6.update_layout(
                                            title=f"{selected_pref}ã®å¸‚åŒºç”ºæ‘åˆ¥ã€Œ{selected_keyword_ts}ã€å‡ºç¾å›æ•°æ¨ç§»{title_suffix}",
                                            xaxis_title="å¹´åº¦",
                                            yaxis_title="å‡ºç¾å›æ•°",
                                            hovermode='x unified',
                                            height=500
                                        )
                                        
                                        fig6.update_xaxes(
                                            tickformat='d',
                                            dtick=1,
                                            tickmode='linear'
                                        )
                                        
                                        st.plotly_chart(fig6, use_container_width=True)
                                        
                                        # é¸æŠã—ãŸå¸‚åŒºç”ºæ‘ã®çµ±è¨ˆæƒ…å ±
                                        with st.expander("é¸æŠã—ãŸå¸‚åŒºç”ºæ‘ã®çµ±è¨ˆæƒ…å ±"):
                                            # å„å¸‚åŒºç”ºæ‘ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—
                                            stats_data = []
                                            
                                            # é¸æŠã—ãŸéƒ½é“åºœçœŒã®ãƒ‡ãƒ¼ã‚¿ã§å†è¨ˆç®—
                                            df_pref_for_stats = df_ts[df_ts['prefecture_name'] == selected_pref]
                                            
                                            for muni in selected_munis:
                                                muni_data = df_pref_for_stats[df_pref_for_stats['municipality_name'] == muni]
                                                
                                                if len(muni_data) > 0:
                                                    # ç·ãƒšãƒ¼ã‚¸æ•°
                                                    total_pages = len(muni_data)
                                                    
                                                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒšãƒ¼ã‚¸æ•°ï¼ˆhas_keywordãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ï¼‰
                                                    pages_with_kw = muni_data['has_keyword'].sum()
                                                    
                                                    # ãƒšãƒ¼ã‚¸å‰²åˆ
                                                    page_ratio = (pages_with_kw / total_pages * 100) if total_pages > 0 else 0
                                                    
                                                    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç·å‡ºç¾å›æ•°
                                                    total_kw_count = muni_data['keyword_count'].sum()
                                                    
                                                    # å¹´åº¦ç¯„å›²
                                                    valid_years = pivot_data[pivot_data[muni] > 0].index if muni in pivot_data.columns else []
                                                    first_year = int(valid_years.min()) if len(valid_years) > 0 else '-'
                                                    last_year = int(valid_years.max()) if len(valid_years) > 0 else '-'
                                                    
                                                    stats_data.append({
                                                        'å¸‚åŒºç”ºæ‘': muni,
                                                        'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°': int(total_kw_count),
                                                        'ç·ãƒšãƒ¼ã‚¸æ•°': int(total_pages),
                                                        'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒšãƒ¼ã‚¸æ•°': int(pages_with_kw),
                                                        'ãƒšãƒ¼ã‚¸å‰²åˆ(%)': round(page_ratio, 1),
                                                        'æœ€åˆã®å¹´åº¦': first_year,
                                                        'æœ€å¾Œã®å¹´åº¦': last_year
                                                    })
                                            
                                            if stats_data:
                                                stats_df = pd.DataFrame(stats_data)
                                                
                                                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆ
                                                stats_df = stats_df.sort_values('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°', ascending=False)
                                                
                                                st.dataframe(
                                                    stats_df,
                                                    use_container_width=True,
                                                    hide_index=True,
                                                    column_config={
                                                        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾å›æ•°": st.column_config.NumberColumn(format="%d"),
                                                        "ç·ãƒšãƒ¼ã‚¸æ•°": st.column_config.NumberColumn(format="%d"),
                                                        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒšãƒ¼ã‚¸æ•°": st.column_config.NumberColumn(format="%d"),
                                                        "ãƒšãƒ¼ã‚¸å‰²åˆ(%)": st.column_config.NumberColumn(format="%.1f%%")
                                                    }
                                                )
                                    else:
                                        st.warning("è¡¨ç¤ºã™ã‚‹å¸‚åŒºç”ºæ‘ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
                                else:
                                    st.info(f"{selected_pref}ã®å¸‚åŒºç”ºæ‘åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                            else:
                                st.info(f"{selected_pref}ã«ã¯ã€Œ{selected_keyword_ts}ã€ã®å¹´åº¦åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                        else:
                            st.info(f"{selected_pref}ã«ã¯ã€Œ{selected_keyword_ts}ã€ã®å‡ºç¾ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        st.info(f"{selected_pref}ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    with tab4:
        st.header("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ")
        
        st.markdown("""
        ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã—ã¦å˜èªã«åˆ†è§£ã—ã€åŒã˜ãƒšãƒ¼ã‚¸ã«å‡ºç¾ã™ã‚‹å˜èªã®å…±èµ·é–¢ä¿‚ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
        """)
        
        # AIæŠ½å‡ºã®èª¬æ˜
        if OPENAI_AVAILABLE:
            with st.expander("AIæŠ½å‡ºæ©Ÿèƒ½ã«ã¤ã„ã¦"):
                st.markdown("""
                **AIã«ã‚ˆã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã‚’ä½¿ç”¨ã™ã‚‹ã¨ï¼š**
                - OpenAI APIã‚’ä½¿ç”¨ã—ã¦ã€å„æ–‡æ›¸ã‹ã‚‰ç‰¹å¾´çš„ã§é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‡ªå‹•æŠ½å‡ºã—ã¾ã™
                - ä¸€èˆ¬çš„ãªå˜èªã‚„è¡Œæ”¿ç”¨èªã‚’é™¤å¤–ã—ã€ã‚ˆã‚Šç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã§ãã¾ã™
                - è¤‡åˆèªï¼ˆä¾‹ï¼šã€Œåœ°åŸŸæ´»æ€§åŒ–ã€ã€Œãƒ‡ã‚¸ã‚¿ãƒ«ç”°åœ’éƒ½å¸‚ã€ï¼‰ã‚‚é©åˆ‡ã«æŠ½å‡ºã•ã‚Œã¾ã™
                - å‡¦ç†æ™‚é–“ã¯é•·ããªã‚Šã¾ã™ãŒã€ã‚ˆã‚Šè³ªã®é«˜ã„å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒä½œæˆã§ãã¾ã™
                
                **é€šå¸¸ã®å½¢æ…‹ç´ è§£æã§ã¯ï¼š**
                - MeCabã¾ãŸã¯ç°¡æ˜“çš„ãªå½¢æ…‹ç´ è§£æã‚’ä½¿ç”¨ã—ã¾ã™
                - é«˜é€Ÿã«å‡¦ç†ã§ãã¾ã™ãŒã€ä¸€èˆ¬çš„ãªå˜èªã‚‚å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
                """)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
        col1, col2, col3 = st.columns(3)
        with col1:
            filter_pref_net = st.selectbox(
                "éƒ½é“åºœçœŒã§ãƒ•ã‚£ãƒ«ã‚¿",
                options=['å…¨å›½'] + sorted(df['prefecture_name'].dropna().unique().tolist()),
                key="filter_pref_net"
            )
        with col2:
            filter_year_net = st.selectbox(
                "å¹´åº¦ã§ãƒ•ã‚£ãƒ«ã‚¿",
                options=['ã™ã¹ã¦'] + sorted(df['fiscal_year'].dropna().unique().tolist()),
                key="filter_year_net"
            )
        with col3:
            filter_muni_net = st.selectbox(
                "å¸‚åŒºç”ºæ‘ã§ãƒ•ã‚£ãƒ«ã‚¿",
                options=['ã™ã¹ã¦'] + (
                    sorted(df[df['prefecture_name'] == filter_pref_net]['municipality_name'].dropna().unique().tolist())
                    if filter_pref_net != 'å…¨å›½' else []
                ),
                key="filter_muni_net",
                disabled=(filter_pref_net == 'å…¨å›½')
            )
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        st.subheader("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        col1, col2, col3 = st.columns(3)
        with col1:
            min_count = st.slider("æœ€å°å‡ºç¾å›æ•°", min_value=1, max_value=50, value=5, 
                                help="ã“ã®å›æ•°ä»¥ä¸Šå‡ºç¾ã™ã‚‹å˜èªã®ã¿ã‚’è¡¨ç¤º")
        with col2:
            top_n_words = st.slider("è¡¨ç¤ºã™ã‚‹å˜èªæ•°", min_value=10, max_value=200, value=50, step=10,
                                  help="å‡ºç¾é »åº¦ã®é«˜ã„å˜èªã‹ã‚‰ä¸Šä½Nå€‹ã‚’è¡¨ç¤º")
        with col3:
            layout_type = st.selectbox("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ", 
                                      options=['spring', 'circular', 'kamada_kawai'],
                                      help="ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é…ç½®æ–¹æ³•")
        
        # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        st.subheader("ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºè¨­å®š")
        col1, col2 = st.columns(2)
        with col1:
            community_resolution = st.slider(
                "ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è§£åƒåº¦",
                min_value=0.1,
                max_value=3.0,
                value=1.0,
                step=0.1,
                help="å€¤ãŒå¤§ãã„ã»ã©å°ã•ãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«åˆ†å‰²ã•ã‚Œã¾ã™"
            )
        with col2:
            edge_threshold = st.slider(
                "ã‚¨ãƒƒã‚¸è¡¨ç¤ºé–¾å€¤",
                min_value=0.0,
                max_value=1.0,
                value=0.3,
                step=0.1,
                help="å¼±ã„å…±èµ·é–¢ä¿‚ã®ã‚¨ãƒƒã‚¸ã‚’éè¡¨ç¤ºã«ã—ã¾ã™"
            )
        
        # AIä½¿ç”¨æ™‚ã®è¿½åŠ è¨­å®š
        if use_ai_extraction and openai_api_key:
            col1, col2 = st.columns(2)
            with col1:
                sample_size = st.number_input(
                    "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ï¼ˆ0=å…¨ãƒ‡ãƒ¼ã‚¿ï¼‰",
                    min_value=0,
                    max_value=len(df),
                    value=min(300, len(df)),
                    step=50,
                    help="AIå‡¦ç†ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™"
                )
            with col2:
                estimated_cost = (sample_size if sample_size > 0 else len(df)) * 0.004
                st.info(f"æ¨å®šAPIæ–™é‡‘: ç´„${estimated_cost:.2f}")
        else:
            sample_size = 0
        
        if st.button("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆ", type="primary"):
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            df_filtered_net = df.copy()
            if filter_pref_net != 'å…¨å›½':
                df_filtered_net = df_filtered_net[df_filtered_net['prefecture_name'] == filter_pref_net]
            if filter_year_net != 'ã™ã¹ã¦':
                df_filtered_net = df_filtered_net[df_filtered_net['fiscal_year'] == filter_year_net]
            if filter_muni_net != 'ã™ã¹ã¦':
                df_filtered_net = df_filtered_net[df_filtered_net['municipality_name'] == filter_muni_net]
            
            if len(df_filtered_net) == 0:
                st.warning("ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                # AIä½¿ç”¨æ™‚ã®APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
                if use_ai_extraction and not openai_api_key:
                    st.error("OpenAI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                else:
                    with st.spinner("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆä¸­..."):
                        # å…±èµ·é »åº¦ã‚’è¨ˆç®—
                        word_counts, cooccurrence_data, top_words = calculate_cooccurrence(
                            df_filtered_net, 
                            min_count=min_count, 
                            top_n_words=top_n_words,
                            use_ai=use_ai_extraction,
                            api_key=openai_api_key,
                            sample_size=sample_size if use_ai_extraction else None
                        )
                        
                        if not cooccurrence_data:
                            st.warning("å…±èµ·é–¢ä¿‚ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€å°å‡ºç¾å›æ•°ã‚’ä¸‹ã’ã¦ã¿ã¦ãã ã•ã„ã€‚")
                        else:
                            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œæˆ
                            traces, stats, G, partition = create_cooccurrence_network(
                                word_counts, 
                                cooccurrence_data, 
                                top_words, 
                                layout_type,
                                community_resolution,
                                edge_threshold
                            )
                            
                            # Plotlyã§ã®å¯è¦–åŒ–
                            fig_net = go.Figure(data=traces)
                            
                            fig_net.update_layout(
                                title=f"å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ{stats['num_nodes']}å˜èªã€{stats['num_edges']}å…±èµ·é–¢ä¿‚ã€{stats['num_communities']}ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼‰",
                                showlegend=True,
                                hovermode='closest',
                                margin=dict(b=20,l=5,r=5,t=40),
                                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                height=800,
                                plot_bgcolor='rgba(240,240,240,0.1)'
                            )
                            
                            st.plotly_chart(fig_net, use_container_width=True)
                            
                            # çµ±è¨ˆæƒ…å ±
                            st.subheader("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ")
                            col1, col2, col3, col4, col5 = st.columns(5)
                            with col1:
                                st.metric("ãƒãƒ¼ãƒ‰æ•°", stats['num_nodes'])
                            with col2:
                                st.metric("ã‚¨ãƒƒã‚¸æ•°", stats['num_edges'])
                            with col3:
                                st.metric("ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ•°", stats['num_communities'])
                            with col4:
                                st.metric("å¹³å‡æ¬¡æ•°", f"{stats['avg_degree']:.2f}")
                            with col5:
                                st.metric("å¯†åº¦", f"{stats['density']:.3f}")
                            
                            # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£åˆ¥ã®å˜èªãƒªã‚¹ãƒˆ
                            with st.expander("ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£åˆ¥å˜èªãƒªã‚¹ãƒˆ"):
                                communities_dict = defaultdict(list)
                                for node, comm_id in partition.items():
                                    communities_dict[comm_id].append((node, word_counts[node]))
                                
                                for comm_id in sorted(communities_dict.keys()):
                                    members = sorted(communities_dict[comm_id], key=lambda x: x[1], reverse=True)
                                    st.write(f"**ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ {comm_id}** ({len(members)}å˜èª)")
                                    member_text = ", ".join([f"{word}({count})" for word, count in members[:10]])
                                    if len(members) > 10:
                                        member_text += f"... ä»–{len(members)-10}å˜èª"
                                    st.write(member_text)
                                    st.write("")
                            
                            # é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°
                            with st.expander("é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½20ï¼‰"):
                                # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¦ã‚½ãƒ¼ãƒˆ
                                word_score_list = []
                                for word, count in word_counts.items():
                                    if word in top_words:
                                        score = calculate_word_importance_score(word, count, len(df_filtered_net))
                                        word_score_list.append((word, count, score))
                                
                                # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
                                word_score_list.sort(key=lambda x: x[2], reverse=True)
                                
                                top_words_df = pd.DataFrame([
                                    {'å˜èª': word, 'å‡ºç¾å›æ•°': count, 'é‡è¦åº¦ã‚¹ã‚³ã‚¢': f"{score:.2f}"}
                                    for word, count, score in word_score_list[:20]
                                ])
                                st.dataframe(top_words_df, use_container_width=True, hide_index=True)
                            
                            # å…±èµ·é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                            with st.expander("å…±èµ·é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆä¸Šä½20ï¼‰"):
                                cooccur_df = pd.DataFrame([
                                    {'å˜èª1': word1, 'å˜èª2': word2, 'å…±èµ·å›æ•°': count}
                                    for (word1, word2), count in sorted(cooccurrence_data.items(), key=lambda x: x[1], reverse=True)[:20]
                                ])
                                st.dataframe(cooccur_df, use_container_width=True, hide_index=True)
                            
                            # ä½¿ç”¨ã—ãŸæŠ½å‡ºæ–¹æ³•ã‚’è¡¨ç¤º
                            extraction_method = "AIæŠ½å‡º" if use_ai_extraction else ("MeCab" if MECAB_AVAILABLE else "ç°¡æ˜“å½¢æ…‹ç´ è§£æ")
                            st.info(f"ä½¿ç”¨ã—ãŸæŠ½å‡ºæ–¹æ³•: {extraction_method}")
                            
                            # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                            st.session_state.cooccurrence_data = {
                                'word_counts': word_counts,
                                'cooccurrence': cooccurrence_data,
                                'top_words': top_words,
                                'graph': G
                            }